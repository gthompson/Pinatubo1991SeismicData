{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "#from obspy import UTCDateTime #, read, read_events\n",
    "#from obspy.core.event import Comment #Event, Origin, Arrival, Pick, WaveformStreamID, Catalog\n",
    "\n",
    "SOURCE_DIR = '/data/Pinatubo/PHASE'\n",
    "REPO_DIR = '/home/thompsong/Developer/Pinatubo1991SeismicData'\n",
    "seisan_db_name = 'PNTBO'\n",
    "SEISAN_TOP = '/data/SEISAN_DB'\n",
    "REA_DIR = os.path.join(SEISAN_TOP, 'REA', seisan_db_name)\n",
    "WAV_DIR = os.path.join(SEISAN_TOP, 'WAV', seisan_db_name)\n",
    "DEV_DIR = '/home/thompsong/Developer'\n",
    "LIB_DIR = os.path.join(DEV_DIR, 'SoufriereHillsVolcano', 'lib')\n",
    "sys.path.append(LIB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create (or load) a wfdisc table and corresponding catalog\n",
    "import glob\n",
    "from libseisGT import index_waveformfiles\n",
    "import pandas as pd\n",
    "from obspy import read_events\n",
    "wfdisc_csv = os.path.join(REPO_DIR, 'metadata', 'wfdisc_catalog.csv')\n",
    "wfdiscqml = os.path.join(REPO_DIR, 'metadata', 'wfdisc_catalog.xml')\n",
    "if os.path.isfile(wfdisc_csv) and os.path.isfile(wfdiscqml):\n",
    "    wfdiscdf = pd.read_csv(wfdisc_csv)\n",
    "    for col in ['starttime', 'endtime']:\n",
    "        wfdiscdf[col] = wfdiscdf[col].apply(lambda x: UTCDateTime(x) if pd.notna(x) else None)\n",
    "    wfdisc_catalog = read_events(wfdiscqml)\n",
    "else:\n",
    "    # create a list of all wavfiles in the Seisan WAV database\n",
    "    # Define the file pattern for Seisan WAV files\n",
    "\n",
    "    print(f'got {len(wav_files)} wav files')    \n",
    "    file_pattern = os.path.join(WAV_DIR, \"*\", \"*\", f\"*M.{seisan_db_name}_*\")\n",
    "    wav_files = sorted(glob.glob(file_pattern))   \n",
    "    wfdiscdf, wfdisc_catalog = index_waveformfiles(wav_files, ampeng=True, events=True)\n",
    "    wfdiscdf.to_csv(wfdisc_csv, index=None)\n",
    "    wfdisc_catalog.write(wfdiscqml, format=\"QUAKEML\")\n",
    "    print(f\"\\n✅ Saved wfdisc catalog as {wfdiscqml}\")\n",
    "\n",
    "print(f'The wfdisc catalog has {wfdisc_catalog.count()} events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy import UTCDateTime\n",
    "\n",
    "t0 = UTCDateTime()\n",
    "\n",
    "# Ensure df has a 'starttime' column in UTCDateTime format\n",
    "df_grouped = wfdiscdf.groupby('file_index').first().reset_index()  # Keep unique start times\n",
    "\n",
    "# Extract date from starttime\n",
    "#df_grouped['starttime'] = df_grouped['starttime'].apply(lambda x: UTCDateTime(x))  # Convert to date (YYYY-MM-DD)\n",
    "df_grouped['date'] = df_grouped['starttime'].apply(lambda x: x.date())  # Convert to date (YYYY-MM-DD)\n",
    "\n",
    "# Count unique events per day\n",
    "daily_counts = df_grouped['date'].value_counts().sort_index()\n",
    "\n",
    "# Compute cumulative sum of events\n",
    "cumulative_counts = daily_counts.cumsum()\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Bar plot for daily event counts\n",
    "ax1.bar(daily_counts.index, daily_counts.values, color='blue', alpha=0.6, width=1.0, label='Events per Day')\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax1.set_ylabel(\"Number of Event Files per Day\", color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "#ax1.set_title(\"Number of Unique Events per Day and Cumulative Count\")\n",
    "\n",
    "# Create second y-axis for cumulative events\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(cumulative_counts.index, cumulative_counts.values, color='green', linestyle='-', linewidth=2, label='Cumulative Events')\n",
    "ax2.set_ylabel(\"Cumulative Number of Event Files\", color='green')\n",
    "ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "# Add vertical dashed red line for eruption\n",
    "eruption_time = UTCDateTime(\"1991-06-15T13:42:00\").date\n",
    "ax1.axvline(x=eruption_time, color='red', linestyle='--', linewidth=2, label='Eruption')\n",
    "\n",
    "# Add annotation for eruption\n",
    "ax1.text(eruption_time, ax1.get_ylim()[1] * 0.9, \"Eruption\", color='red', fontsize=12, rotation=90, verticalalignment='top')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Grid and legend\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "#fig.legend(loc=\"upper left\", bbox_to_anchor=(0.1, 0.9))\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "t1 = UTCDateTime()\n",
    "print(f'grouping took {t1-t0} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to very roughly estimate network magnitude from amplitudes, and from energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "'''\n",
    "try:\n",
    "    df_grouped['starttime'] = pd.to_datetime([x.datetime for x in df_grouped['starttime']])\n",
    "except:\n",
    "    pass\n",
    "'''\n",
    "df_grouped['sta_mag_e'] = (np.log10(df_grouped['energy'])/2) - 1.0\n",
    "df_grouped['sta_mag_a'] = np.log10(df_grouped['amplitude'])\n",
    "display(df_grouped)\n",
    "\n",
    "\n",
    "# Compute network magnitudes as the nanmedian of 'sta_mag_a' and 'sta_mag_e' per event (grouped by 'starttime')\n",
    "df_network_mag = df_grouped.groupby('starttime', as_index=False).agg({\n",
    "    'sta_mag_a': lambda x: np.nanmedian(x),\n",
    "    'sta_mag_e': lambda x: np.nanmedian(x)\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "df_network_mag.rename(columns={'sta_mag_a': 'network_mag_a', 'sta_mag_e': 'network_mag_e'}, inplace=True)\n",
    "\n",
    "display(df_network_mag)\n",
    "\n",
    "df_network_mag.plot(x='network_mag_a', y='network_mag_e')\n",
    "\n",
    "# Drop rows with NaN values to ensure valid regression\n",
    "df_network_mag_filtered = df_network_mag[df_network_mag['network_mag_a'] >= -2].reset_index(drop=True)\n",
    "df_regression = df_network_mag_filtered.dropna(subset=['network_mag_a', 'network_mag_e'])\n",
    "\n",
    "# Perform linear regression\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(\n",
    "    df_regression['network_mag_a'], df_regression['network_mag_e']\n",
    ")\n",
    "\n",
    "# Generate regression line\n",
    "x_vals = np.linspace(df_regression['network_mag_a'].min(), df_regression['network_mag_a'].max(), 100)\n",
    "y_vals = slope * x_vals + intercept\n",
    "\n",
    "# Plot data and regression line\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df_regression['network_mag_a'], df_regression['network_mag_e'], color='blue', alpha=0.6, label=\"Data\")\n",
    "plt.plot(x_vals, y_vals, color='red', label=f\"y = {slope:.2f}x + {intercept:.2f}\\nR² = {r_value**2:.2f}\")\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Network Magnitude A\")\n",
    "plt.ylabel(\"Network Magnitude E\")\n",
    "plt.title(\"Linear Regression: Network Mag E vs. Network Mag A\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Display regression results\n",
    "regression_results = pd.DataFrame({\n",
    "    \"Slope\": [slope],\n",
    "    \"Intercept\": [intercept],\n",
    "    \"R-squared\": [r_value**2],\n",
    "    \"p-value\": [p_value],\n",
    "    \"Std Error\": [std_err]\n",
    "})\n",
    "\n",
    "display(regression_results)\n",
    "\n",
    "df_regression.plot(x='starttime', y='network_mag_a', style='.')\n",
    "df_regression.plot(x='starttime', y='network_mag_e', style='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wfdisc to Catalog\n",
    "The objective now is to build a catalog using the wfdisc table as a starting point\n",
    "\n",
    "Expand a wfdisc table to include an ObsPy ResourceIdentifier for each row (called 'waveform_id'), to identify the waveform data for that Trace, and also another ResourceIdentifier that identifies the event. But in that case, should I just be creating a Catalog, since that has ResourceIdentifier's built-in? I will sometimes want to create a wfdisc for continuous data that do not represent events, so should the event_id column be blank in that case?\n",
    "\n",
    "1. Read hypo71 summary files into a Catalog object\n",
    "2. Read monthly phase files into a Catalog object, associating picks with existing events, otherwise creating new events\n",
    "3. Loop through wfdisc table. Associate each row with an existing pick. Add a reference to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from obspy import read_events\n",
    "from libseisGT import parse_hypo71_file\n",
    "hypo71qml = os.path.join(REPO_DIR, 'metadata', 'hypo71_catalog.xml')\n",
    "if os.path.isfile(hypo71qml):\n",
    "    hypo71_catalog = read_events(hypo71qml)\n",
    "\n",
    "else:\n",
    "    # read hypo71 file\n",
    "    file_path = os.path.join(SOURCE_DIR,\"Locations\",\"Pinatubo_all.sum\")  # Replace with actual file\n",
    "    hypo71_catalog, unparsed_lines = parse_hypo71_file(file_path)\n",
    "\n",
    "    # Print parsed events\n",
    "    for event in hypo71_catalog:\n",
    "        print(f\"Time: {event.origins[0].time}, Lat: {event.origins[0].latitude}, Lon: {event.origins[0].longitude}, \"\n",
    "            f\"Depth: {event.origins[0].depth}m, Mag: {event.magnitudes[0].mag}, \"\n",
    "            f\"n_ass: {event.origins[0].comments[0]}, Time Residual: {event.origins[0].comments[1]}\")\n",
    "\n",
    "    hypo71_catalog.write(hypo71qml, format=\"QUAKEML\")\n",
    "    print(f\"\\n✅ Saved hypo71 catalog as {hypo71qml}\")\n",
    "print(f'The hypo71 catalog has {hypo71_catalog.count()} events')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse monthly phase files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from obspy import UTCDateTime\n",
    "from obspy.core.event import Event, Origin, Arrival, Pick, WaveformStreamID, Catalog\n",
    "\n",
    "def parse_monthly_phasefile(file_path, catalog, error_log=\"lines_that_could_not_be_parsed.txt\", seconds=30):\n",
    "    \"\"\"\n",
    "    Parses seismic phase data using **fixed character positions**.\n",
    "    Creates an ObsPy Event object for each set of picks, separated by a line with just '10'.\n",
    "    **Fixed-width character slicing ensures reliable parsing**\n",
    "    \"\"\"\n",
    "    current_event = []  # Temporary storage for arrivals in the current event\n",
    "    print(catalog)\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file, open(error_log, 'a', encoding='utf-8') as error_file:\n",
    "        for line in file:\n",
    "\n",
    "            line = line.rstrip()  # Strip \\n and spaces\n",
    "\n",
    "            # DEBUG: Print each line to confirm correct reading\n",
    "            #print(f\"DEBUG: Raw line -> '{line}'\")\n",
    "\n",
    "            # Print all positions where 'S' appears\n",
    "            s_positions = [i for i, char in enumerate(line) if char == 'S']\n",
    "            # Remove any positions outside the range [30, 40]\n",
    "            s_positions = [pos for pos in s_positions if 35 <= pos <= 40]\n",
    "            s_pos = 0\n",
    "            if len(s_positions)==1:\n",
    "                s_pos = s_positions[0]\n",
    "            #print(f\"DEBUG: 'S' found at positions -> {s_pos} in line: {repr(line)}\")\n",
    "\n",
    "            # If a \"10\" separator is found, start a new event\n",
    "            if line.strip() == \"10\":\n",
    "                if current_event:  # Save the previous event if it has arrivals\n",
    "                    obspyevents = create_obspy_event(current_event)\n",
    "                    catalog.events.extend(obspyevents) \n",
    "                    current_event = []  # Reset for the next event\n",
    "                continue  # Skip processing the separator itself\n",
    "\n",
    "            #  **Fixed Character Positions**\n",
    "            station = line[0:3].strip()  # Positions 1-3\n",
    "            if station.lower()=='xxx' or len(station)<3:\n",
    "                continue\n",
    "            orientation = line[3:4].strip()  # Position 4\n",
    "            if orientation=='P':\n",
    "                line = line[0:4] + '  ' + line[4:]\n",
    "            p_arrival_code = line[4:8].replace(' ', '?')   # Positions 5-8\n",
    "            if line[8]==' ':\n",
    "                timestamp_str = line[9:24].strip().replace(\" \", \"0\")  # Positions 10-24 (date/time)\n",
    "            else:\n",
    "                timestamp_str = line[8:23].strip().replace(\" \", \"0\")  # Positions 10-24 (date/time)                \n",
    "            s_wave_delay = line[s_pos-7:s_pos-1].strip() if s_pos else \"\"\n",
    "            if s_pos>0:\n",
    "                if len(line)>s_pos+3:\n",
    "                    s_arrival_code = line[s_pos-1:s_pos+3].replace(' ', '?')\n",
    "                else:\n",
    "                    s_arrival_code = line[s_pos-1:].ljust(4).replace(' ', '?')\n",
    "            else:\n",
    "                s_arrival_code = \"\"  \n",
    "            unknown_str = line[42:].strip() if len(line) > 47 else \"\"\n",
    "            unknown_str = re.sub(r'[^\\x20-\\x7E]', '', unknown_str).strip()  # Keeps only printable ASCII characters\n",
    "\n",
    "            #  **Check if P-wave data exists (only if 'P' in position 6, index 5)**\n",
    "            has_p_wave = len(p_arrival_code) >= 2 and p_arrival_code[1] == \"P\"\n",
    "\n",
    "            #  **Check if S-wave data exists (only if 'S' in position 2, index 1 of S-wave code)**\n",
    "            has_s_wave = s_pos>0\n",
    "\n",
    "            # Convert spaces and '?' in arrival codes to 'unknown'\n",
    "            p_arrival_code = p_arrival_code.replace(\"?\", \" \") if has_p_wave else \"unknown\"\n",
    "            s_arrival_code = s_arrival_code.replace(\"?\", \" \") if has_s_wave else \"unknown\"\n",
    "\n",
    "            # Convert timestamp to UTC\n",
    "            add_secs = 0\n",
    "            if timestamp_str[-5:]=='60.00':\n",
    "                timestamp_str = timestamp_str.replace('60.00', '00.00')\n",
    "                add_secs += 60\n",
    "            if timestamp_str[-7:-5]=='60':\n",
    "                timestamp_str = timestamp_str.replace('60', '00')\n",
    "                add_secs += 3600                \n",
    "            try:\n",
    "                timestamp = UTCDateTime(datetime.strptime(timestamp_str, \"%y%m%d%H%M%S.%f\"))\n",
    "            except:\n",
    "                try:\n",
    "                    timestamp = UTCDateTime(datetime.strptime(timestamp_str, \"%y%m%d%H%M\"))\n",
    "                except:\n",
    "                    continue\n",
    "            timestamp = timestamp + add_secs\n",
    "\n",
    "            #  **Determine SEED channel**\n",
    "            if orientation in \"ZNE\":  # Standard orientations\n",
    "                channel = f\"EH{orientation}\"\n",
    "            elif orientation == \"L\":  # Special case for \"L\"\n",
    "                channel = \"ELZ\"\n",
    "            else:\n",
    "                channel = f'??{orientation}'\n",
    "                #raise ValueError(f\"Unknown orientation '{orientation}' in '{station}'\")\n",
    "\n",
    "            # Construct SEED ID\n",
    "            seed_id = f\"XB.{station}..{channel}\"\n",
    "\n",
    "            #  **Store P-wave arrival**\n",
    "            if has_p_wave:\n",
    "                p_arrival = {\n",
    "                    \"seed_id\": seed_id,\n",
    "                    \"time\": timestamp,\n",
    "                    \"onset\": p_arrival_code[0] if p_arrival_code[0] in [\"I\", \"E\"] else \"unknown\",\n",
    "                    \"type\": \"P\",\n",
    "                    \"first_motion\": p_arrival_code[2] if p_arrival_code[2] in [\"U\", \"D\"] else \"unknown\",\n",
    "                    \"uncertainty\": int(p_arrival_code[3]) if p_arrival_code[3].isdigit() else None,\n",
    "                    \"unknown\": unknown_str\n",
    "                }\n",
    "                current_event.append(p_arrival)\n",
    "\n",
    "            #  **Store S-wave arrival**\n",
    "            if has_s_wave and s_wave_delay.replace(\".\", \"\").isdigit():\n",
    "                s_wave_time = timestamp + timedelta(seconds=float(s_wave_delay))\n",
    "                s_arrival = {\n",
    "                    \"seed_id\": seed_id,\n",
    "                    \"time\": s_wave_time,\n",
    "                    \"onset\": s_arrival_code[0] if s_arrival_code[0] in [\"I\", \"E\"] else \"unknown\",\n",
    "                    \"type\": \"S\",\n",
    "                    \"first_motion\": s_arrival_code[2] if s_arrival_code[2] in [\"U\", \"D\"] else \"unknown\",\n",
    "                    \"uncertainty\": int(s_arrival_code[3]) if s_arrival_code[3].isdigit() else None,\n",
    "                    \"unknown\": unknown_str\n",
    "                }\n",
    "                current_event.append(s_arrival)\n",
    "\n",
    "            '''\n",
    "            except Exception as e:\n",
    "                error_file.write(f\"Error parsing line: {line}\\nReason: {e}\\n\")\n",
    "                error_file.write(f'{station}, {orientation}, {p_arrival_code}, {timestamp_str}, {s_wave_delay}, {s_arrival_code}, {optional_number}'+'\\n\\n')\n",
    "                continue  # Skip this line and move to the next\n",
    "            '''\n",
    "\n",
    "    # Save the last event if there are remaining arrivals\n",
    "    if current_event:\n",
    "        obspyevents = create_obspy_event(current_event, seconds=seconds)\n",
    "        catalog.events.extend(obspyevents)\n",
    "\n",
    "def create_obspy_event(current_event, seconds=30):\n",
    "    \"\"\"\n",
    "    Saves an event (list of arrivals) to an ObsPy Event object.\n",
    "    \n",
    "    Returns:\n",
    "        obspy.core.event.Event: The ObsPy Event object.\n",
    "    \"\"\"\n",
    "\n",
    "    picks = []  # List to store Pick objects\n",
    "\n",
    "    events = []\n",
    "\n",
    "    for arrival in current_event:\n",
    "\n",
    "        seed_id = arrival['seed_id']\n",
    "        pick_time = UTCDateTime(arrival['time'])\n",
    "        onset = arrival['onset'] if arrival['onset'] != \"unknown\" else None\n",
    "        if onset=='E':\n",
    "            onset = 'emergent'\n",
    "        elif onset=='I':\n",
    "            onset='impulsive'\n",
    "        else:\n",
    "            onset=='questionable'\n",
    "        phase_hint = arrival['type']  # \"P\" or \"S\"\n",
    "        polarity = arrival['first_motion'] if arrival['first_motion'] != \"unknown\" else None\n",
    "        if polarity=='U':\n",
    "            polarity='positive'\n",
    "        elif polarity=='D':\n",
    "            polarity='negative'\n",
    "        else:\n",
    "            polarity='undecidable'\n",
    "        uncertainty = float(arrival['uncertainty']) if arrival['uncertainty'] else None\n",
    "\n",
    "        # Create a Pick object\n",
    "        pick = Pick(\n",
    "            time=pick_time,\n",
    "            waveform_id=WaveformStreamID(seed_string=seed_id),\n",
    "            onset=onset,\n",
    "            phase_hint=phase_hint,\n",
    "            polarity=polarity\n",
    "        )\n",
    "        picks.append(pick)\n",
    "    \n",
    "    # check that picks are within (say) 30-seconds of each other. arrange into groups\n",
    "    grouped_picks = group_picks(picks, seconds=seconds)\n",
    "\n",
    "    for group in grouped_picks:\n",
    "        event = Event()  # Create an empty ObsPy Event\n",
    "        arrivals = []\n",
    "\n",
    "        for pick in group:\n",
    "\n",
    "            # Create an Arrival object (linking to the Pick)\n",
    "            arrival = Arrival(\n",
    "                pick_id=pick.resource_id,\n",
    "                phase=phase_hint,\n",
    "                time_residual=uncertainty if uncertainty is not None else 0.0\n",
    "            )\n",
    "            arrivals.append(arrival)\n",
    "    \n",
    "        # Create an Origin object (hypocenter)\n",
    "        origin_time = min(pick.time for pick in picks)  # Use earliest pick as origin time\n",
    "        origin = Origin(\n",
    "            time=origin_time,\n",
    "            arrivals=arrivals\n",
    "        )\n",
    "\n",
    "        # Add Origin and Picks to Event\n",
    "        event.origins.append(origin)\n",
    "        event.picks.extend(picks)\n",
    "\n",
    "        # append event to list of events (which gets added to full catalog)\n",
    "        events.append(event)\n",
    "\n",
    "    return events\n",
    "\n",
    "def group_picks(picks, seconds=30):\n",
    "    \"\"\"\n",
    "    Groups ObsPy Pick objects into clusters where each pick's time is within 'threshold' seconds of another in the group.\n",
    "\n",
    "    :param picks: List of ObsPy Pick objects\n",
    "    :param threshold: Maximum allowed time difference in seconds\n",
    "    :return: List of lists, where each sublist contains grouped Pick objects\n",
    "    \"\"\"\n",
    "    if not picks:\n",
    "        return []\n",
    "    \n",
    "    # Sort picks by time\n",
    "    picks = sorted(picks, key=lambda p: p.time)\n",
    "    \n",
    "    groups = []\n",
    "    current_group = [picks[0]]\n",
    "\n",
    "    for i in range(1, len(picks)):\n",
    "        if (picks[i].time - current_group[-1].time) <= seconds:\n",
    "            current_group.append(picks[i])\n",
    "        else:\n",
    "            groups.append(current_group)\n",
    "            current_group = [picks[i]]\n",
    "    \n",
    "    # Append last group\n",
    "    groups.append(current_group)\n",
    "\n",
    "    return groups\n",
    "\n",
    "#  **Process all `pin*91.pha` files**\n",
    "SOURCE_DIR = '/data/Pinatubo/PHASE'\n",
    "REPO_DIR = '/home/thompsong/Developer/Pinatubo1991SeismicData'\n",
    "REA_DIR = '/data/SEISAN_DB/REA/PINAT'\n",
    "WAV_DIR = '/data/SEISAN_DB/WAV/PINAT'\n",
    "phasesqml = os.path.join(REPO_DIR, 'metadata', 'phases_catalog.xml')\n",
    "\n",
    "if os.path.isfile(phasesqml):\n",
    "    phases_catalog = read_events(phasesqml)\n",
    "else:\n",
    "    input_files = sorted(glob.glob(os.path.join(SOURCE_DIR,\"pin*91.pha\")))\n",
    "\n",
    "    # Create an empty ObsPy Catalog\n",
    "    catalog = Catalog()\n",
    "\n",
    "    for file in input_files:\n",
    "        print(f\"Processing: {file}\")\n",
    "        parse_monthly_phasefile(file, catalog, seconds=15)\n",
    "\n",
    "    import pickle\n",
    "    with open(\"temp_catalog.pkl\", \"wb\") as f:\n",
    "        pickle.dump(catalog, f)\n",
    "\n",
    "    # Sort the events by the first origin time\n",
    "    phases_catalog = sorted(catalog, key=lambda event: event.origins[0].time if event.origins else None)\n",
    "\n",
    "    # Convert back to an ObsPy Catalog\n",
    "    phases_catalog = catalog.__class__(phases_catalog)\n",
    "\n",
    "    # write to QML\n",
    "    phases_catalog.write(phasesqml, format=\"QUAKEML\")\n",
    "    print(f\"\\n✅ Saved phases catalog as {phasesqml}\")\n",
    "\n",
    "print(f'The phases catalog has {phases_catalog.count()} events')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from obspy import UTCDateTime, read_events\n",
    "from obspy.core.event import Catalog\n",
    "\n",
    "def merge_catalogs_hypo71_phases(hypo71_catalog, phases_catalog, seconds=30):\n",
    "    \"\"\"\n",
    "    Merges two ObsPy Catalog objects: one from HYPO71 and one from pick-based origins.\n",
    "    \"\"\"\n",
    "    mergedcatalog = Catalog(events=[])\n",
    "    merged_events = []\n",
    "    \n",
    "    # merge the hypo71 catalog into the phases catalog\n",
    "\n",
    "    # Create sorted list of pick-based origin times\n",
    "    sorted_pick_time_list = sorted(\n",
    "            [(event.origins[0].time, event) for event in phases_catalog if event.origins], \n",
    "            key=lambda x: x[0]\n",
    "        )\n",
    "    \n",
    "    matched_phases_events = []\n",
    "    unmatched_hypo71_events = []\n",
    "\n",
    "    for hypo_event in hypo71_catalog:\n",
    "        hypo_time = hypo_event.origins[0].time\n",
    "        #print(hypo_time)\n",
    "        \n",
    "        # Find the first picktime event that has an origin time larger than the hypo71 event\n",
    "        matching_event = next((event for pick_time, event in sorted_pick_time_list if pick_time > hypo_time and pick_time < hypo_time + seconds), None)\n",
    "\n",
    "        if matching_event:\n",
    "            merged_event = hypo_event.copy()\n",
    "            merged_event.picks.extend(matching_event.picks)\n",
    "            merged_events.append(merged_event)\n",
    "            matched_phases_events.append(matching_event)\n",
    "        else:\n",
    "            unmatched_hypo71_events.append(hypo_event)\n",
    "\n",
    "    unmatched_phases_events = phases_catalog.events        \n",
    "    for phases_event in phases_catalog.events:\n",
    "        if phases_event in matched_phases_events:\n",
    "            unmatched_phases_events.remove(phases_event)\n",
    "\n",
    "    merged_events.extend(unmatched_hypo71_events)\n",
    "    merged_events.extend(unmatched_phases_events)\n",
    "\n",
    "    merged_events = sorted(merged_events, key=lambda event: event.origins[0].time if event.origins else None)\n",
    "    mergedcatalog = Catalog(events=merged_events) \n",
    "\n",
    "    return mergedcatalog\n",
    "\n",
    "\n",
    "mergedqml1 = os.path.join(REPO_DIR, 'metadata', 'Pinatubo_merged_hypo71_phases.xml')\n",
    "if os.path.isfile(mergedqml1):\n",
    "    mergedcatalog1 = read_events(mergedqml1)\n",
    "else:\n",
    "    mergedcatalog1 = merge_catalogs_hypo71_phases(hypo71_catalog, phases_catalog, seconds=30)\n",
    "    mergedcatalog1.write(mergedqml1, format=\"QUAKEML\")\n",
    "print(f'The merged phases and hypo71 catalog has {mergedcatalog1.count()} events')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_event_by_resource_id(catalog, resource_id):\n",
    "    \"\"\"\n",
    "    Selects an Event object from a Catalog based on the Event.resource_id.\n",
    "\n",
    "    Parameters:\n",
    "        catalog (obspy.core.event.Catalog): The ObsPy Catalog object.\n",
    "        resource_id (str): The resource_id of the Event to find.\n",
    "\n",
    "    Returns:\n",
    "        obspy.core.event.Event or None: The matching Event object, or None if not found.\n",
    "    \"\"\"\n",
    "    for event in catalog:\n",
    "        if str(event.resource_id) == resource_id:\n",
    "            return event\n",
    "    return None\n",
    "\n",
    "def merge_events(event1, event2):\n",
    "    \"\"\"\n",
    "    Merges two ObsPy Event objects into a single Event.\n",
    "\n",
    "    Parameters:\n",
    "        event1 (obspy.core.event.Event): First event.\n",
    "        event2 (obspy.core.event.Event): Second event.\n",
    "\n",
    "    Returns:\n",
    "        obspy.core.event.Event: The merged event.\n",
    "    \"\"\"\n",
    "    merged_event = Event()\n",
    "\n",
    "    # Preserve the primary resource_id from event1\n",
    "    merged_event.resource_id = event1.resource_id\n",
    "\n",
    "    # Merge origins (avoid duplicates)\n",
    "    merged_event.origins = list({orig.resource_id.id: orig for orig in event1.origins + event2.origins}.values())\n",
    "\n",
    "    # Merge magnitudes (avoid duplicates)\n",
    "    merged_event.magnitudes = list({mag.resource_id.id: mag for mag in event1.magnitudes + event2.magnitudes}.values())\n",
    "\n",
    "    # Merge focal mechanisms (if available)\n",
    "    merged_event.focal_mechanisms = list({fm.resource_id.id: fm for fm in event1.focal_mechanisms + event2.focal_mechanisms}.values())\n",
    "\n",
    "    # Merge picks (avoid duplicates)\n",
    "    merged_event.picks = list({pick.resource_id.id: pick for pick in event1.picks + event2.picks}.values())\n",
    "\n",
    "    # Merge comments (combine lists)\n",
    "    merged_event.comments = event1.comments + event2.comments\n",
    "\n",
    "    return merged_event\n",
    "\n",
    "def merge_wfdisc_with_picked_catalog(picked_catalog, df, wfdisc_catalog, seconds=20):\n",
    "    \"\"\"\n",
    "    Associates MiniSEED files from a Seisan WAV database with Pick objects in an ObsPy Catalog.\n",
    "    If a Pick's time falls within a MiniSEED file's time range, a Comment is added to the Event.\n",
    "\n",
    "    Parameters:\n",
    "        catalog (obspy.core.event.Catalog): The ObsPy Catalog containing Events with Picks.\n",
    "        wav_directory (str): Path to the Seisan WAV database (e.g., \"WAV/\").\n",
    "\n",
    "    Returns:\n",
    "        obspy.core.event.Catalog: The updated Catalog with associated MiniSEED file paths in Comments.\n",
    "    \"\"\"\n",
    "\n",
    "    merged_events = []\n",
    "    matched_wfdisc_events = []\n",
    "    bad_events = []\n",
    "    '''\n",
    "    # Convert DataFrame columns to NumPy arrays once (outside the loop)\n",
    "    #starttimes = df[\"starttime\"].to_numpy()\n",
    "    #endtimes = df[\"endtime\"].to_numpy()\n",
    "    #event_ids = df[\"event_id\"].to_numpy()  # Store event IDs as an array\n",
    "    starttimes = df[\"starttime\"].tolist()\n",
    "    endtimes = df[\"endtime\"].tolist()\n",
    "    event_ids = df[\"event_id\"].tolist()\n",
    "    # Iterate through Events & Picks to check time association\n",
    "    for event in picked_catalog:\n",
    "        print(event.origins[0].time)\n",
    "        associated_ids = set()  # Track unique MiniSEED files per event\n",
    "        matching_event = None\n",
    "\n",
    "        t0 = UTCDateTime()\n",
    "    '''\n",
    "    '''\n",
    "    # Convert DataFrame columns to lists once (outside the loop)\n",
    "    starttimes = df[\"starttime\"].tolist()\n",
    "    endtimes = df[\"endtime\"].tolist()\n",
    "    event_ids = df[\"event_id\"].tolist()\n",
    "\n",
    "    # Iterate through Events & Picks to check time association\n",
    "    for event in picked_catalog:\n",
    "        otime = event.origins[0].time\n",
    "        associated_ids = set()  # Track unique MiniSEED files per event\n",
    "        matching_event = None\n",
    "\n",
    "        t0 = UTCDateTime()\n",
    "        \n",
    "        # Extract all pick times for the event and sort them\n",
    "        pick_times = sorted([pick.time for pick in event.picks])\n",
    "        \n",
    "        if not pick_times:\n",
    "            continue  # Skip event if no picks\n",
    "        \n",
    "        # Get the earliest and latest pick times\n",
    "        earliest_pick = pick_times[0]\n",
    "        latest_pick = pick_times[-1]\n",
    "        print(f'otime: {otime}: earliest: {earliest_pick-otime}, latest: {latest_pick-otime}, latest-earliest = {latest_pick-earliest_pick}')\n",
    "\n",
    "        # Instead of checking for each pick, check once per event\n",
    "        matching_ids = [event_ids[i] for i in range(len(starttimes)) \n",
    "                        if starttimes[i] <= latest_pick and endtimes[i] >= earliest_pick]\n",
    "\n",
    "        # Add matching file paths to the set\n",
    "        if matching_ids:\n",
    "            associated_ids.update(matching_ids)\n",
    "\n",
    "        for pick in event.picks:\n",
    "            t01 = UTCDateTime()\n",
    "            pick_time = pick.time  # Extract pick time\n",
    "            t02 = UTCDateTime()\n",
    "            # Use NumPy to find matching indices instead of DataFrame filtering\n",
    "            #matching_indices = np.where((starttimes <= pick_time) & (endtimes >= pick_time))[0]\n",
    "            # List comprehension instead of DataFrame filtering\n",
    "            matching_ids = [event_ids[i] for i in range(len(starttimes)) if starttimes[i] <= pick_time <= endtimes[i]]\n",
    "                    \n",
    "            t03 = UTCDateTime()\n",
    "            # Retrieve matching event IDs\n",
    "            #matching_ids = event_ids[matching_indices].tolist()\n",
    "            t04 = UTCDateTime()\n",
    "            # Add matching file paths to the set\n",
    "            if matching_ids:\n",
    "                associated_ids.update(matching_ids)\n",
    "            t05 = UTCDateTime()\n",
    "            #print(f'{t01-t0}, {t02-t01}, {t03-t02}, {t04-t03}, {t05-t04}')\n",
    "        '''    \n",
    "    # Convert DataFrame columns to lists once (outside the loop)\n",
    "    df = df.sort_values(by='starttime')\n",
    "    starttimes = df[\"starttime\"].tolist()\n",
    "    endtimes = df[\"endtime\"].tolist()\n",
    "    event_ids = df[\"event_id\"].tolist()\n",
    "\n",
    "    # Initialize an index to track how much of the sorted df we’ve processed\n",
    "    start_index = 0  \n",
    "    t0 = UTCDateTime()\n",
    "\n",
    "    # Iterate through Events & Picks to check time association\n",
    "    for event in picked_catalog:\n",
    "        otime = event.origins[0].time\n",
    "        if otime.year!=1991:\n",
    "            continue\n",
    "        associated_ids = set()  # Track unique MiniSEED files per event\n",
    "        matching_event = None\n",
    "        \n",
    "        # Extract all pick times for the event - pick times are not presorted\n",
    "        pick_times = [pick.time for pick in event.picks]    \n",
    "        if len(pick_times)>0:\n",
    "            earliest_pick = min(pick_times)\n",
    "            latest_pick = max(pick_times)\n",
    "            print(f'otime: {otime}: earliest: {earliest_pick-otime}, latest: {latest_pick-otime}, latest-earliest = {latest_pick-earliest_pick}')\n",
    "            if latest_pick - earliest_pick > seconds:\n",
    "                bad_events.append(event)\n",
    "                continue\n",
    "          \n",
    "            # **Early stop optimization**: Remove outdated entries from lists\n",
    "            while start_index < len(starttimes) and endtimes[start_index] < earliest_pick:\n",
    "                start_index += 1  # Move forward past old entries\n",
    "\n",
    "            # Now only check the reduced search space\n",
    "            matching_ids = [event_ids[i] for i in range(start_index, len(starttimes))\n",
    "                            if starttimes[i] <= earliest_pick and endtimes[i] >= latest_pick]\n",
    "\n",
    "            # Add matching file paths to the set\n",
    "            if matching_ids:\n",
    "                associated_ids.update(matching_ids)\n",
    "\n",
    "        # Match origin time instead - this can happen for events with no picks - just a hypocenter \n",
    "        # - or if the pick times did not fall inside the start and end times of a WAV file\n",
    "        if len(associated_ids)==0:\n",
    "            # an origin time could be before the corresponding WAV file starts, depending on the pretrigger window length used\n",
    "            matching_rows = df[(df[\"starttime\"] <= otime + seconds) & (df[\"endtime\"] >= otime)]\n",
    "\n",
    "            # Add matching file paths to the set\n",
    "            matching_ids = matching_rows[\"event_id\"].tolist()\n",
    "            if matching_ids:\n",
    "                associated_ids.update(matching_ids)                           \n",
    "\n",
    "        if associated_ids:\n",
    "            resource_ids = sorted(list(associated_ids))\n",
    "            if isinstance(resource_ids[0], str):\n",
    "                sorted_ids = sorted(resource_ids)\n",
    "            else:\n",
    "                sorted_ids = sorted(resource_ids, key=lambda rid: rid.id)\n",
    "            resource_id_to_find = sorted_ids[-1] # we want the last file in the list, I think\n",
    "            # this event from the picked catalog and from wfdisc_catalog should be merged\n",
    "            matching_event = get_event_by_resource_id(wfdisc_catalog, resource_id_to_find)      \n",
    "            \n",
    "        if matching_event:\n",
    "            merged_event = merge_events(matching_event, event)\n",
    "            merged_events.append(merged_event) # add the merged event to the merged catalog\n",
    "            matched_wfdisc_events.append(matching_event) # track the wfdisc_catalog events that have already been merged\n",
    "        else:\n",
    "            merged_events.append(event) # if no matching_event, we still want to pick this picked_catalog event into the merged catalog\n",
    "        t1 = UTCDateTime()\n",
    "        L = len(merged_events)\n",
    "        print(f'Merged catalog has {L} events, elapsed time: {t1-t0} seconds, {(t1-t0)/L} seconds per event', '\\n')\n",
    "\n",
    "    print('Adding unmatched events from wfdisc_catalog')\n",
    "    for wfdisc_event in wfdisc_catalog.events:\n",
    "        if not wfdisc_event in matched_wfdisc_events:\n",
    "            merged_events.append(wfdisc_event)   \n",
    "\n",
    "    try:\n",
    "        merged_events = sorted(merged_events, key=lambda event: event.origins[0].time if event.origins else None)        \n",
    "    except:\n",
    "        print('Could not sort events')\n",
    "    merged_catalog = Catalog(events=merged_events)  \n",
    "    bad_catalog = Catalog(events=bad_events)    \n",
    "\n",
    "    return merged_catalog, bad_catalog\n",
    "         \n",
    "mergedqml2 = os.path.join(REPO_DIR, 'metadata', 'Pinatubo_merged_wfdisc_hypo71_phases.xml')\n",
    "badqml = os.path.join(REPO_DIR, 'metadata', 'events_with_bad_picks.xml')\n",
    "if os.path.isfile(mergedqml2):\n",
    "    mergedcatalog2 = read_events(mergedqml2)\n",
    "else:\n",
    "    mergedcatalog2, bad_catalog = merge_wfdisc_with_picked_catalog(mergedcatalog1, wfdiscdf, wfdisc_catalog)\n",
    "    mergedcatalog2.write(mergedqml2, format=\"QUAKEML\") \n",
    "    bad_catalog.write(badqml, format='QUAKEML')\n",
    "print(f'The merged wfdisc, phases and hypo71 catalog has {mergedcatalog2.count()} events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bad_catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem with 3748 events that have a pick time spread of more than 20 seconds. I didn't think this was possible, so we will have to go backwards and check where this is arising from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Catalog to REA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from obspy.io.nordic.core import blanksfile, _write_nordic\n",
    "def catalog2rea(catalog, author='gt__', nordic_format='OLD', default_evtype='L'):\n",
    "    \"\"\"\n",
    "    Write a Catalog object to a Seisan REA database\n",
    "    Assumptions:\n",
    "    1. catalog has been created using index_waveformfiles and thus has comments with the wavfile path, and what the corresponding sfile name should be\n",
    "    2. if we merged catalogs, the first origin will be \n",
    "    \"\"\"\n",
    "\n",
    "    for event in catalog:\n",
    "\n",
    "        otime = event.origins[-1].time\n",
    "        evtype = event.event_type\n",
    "        if not evtype:\n",
    "            evtype = default_evtype\n",
    "        ymdir = os.path.join(REA_DIR, otime.strftime('%Y'), otime.strftime('%m'))\n",
    "        os.makedirs(ymdir, exist_ok=True)\n",
    "\n",
    "        wavfiles = []\n",
    "        sfiles = []\n",
    "        for comment in event.comments:\n",
    "            if 'wavfile:' in comment.text:\n",
    "                wavfiles.append(os.path.basename(comment.text.split('wavfile:')[1]))\n",
    "            if 'sfile:' in comment.text:\n",
    "                sfiles.append(comment.text.split('sfile:')[1])\n",
    "\n",
    "        if len(sfiles)>0:\n",
    "            _write_nordic(event, sfiles[-1], userid=author, evtype=evtype, outdir=ymdir, wavefiles=wavfiles, \n",
    "                      explosion=False, nordic_format=nordic_format, overwrite=True, high_accuracy=False)  \n",
    "        else:\n",
    "            sfilename = blanksfile('', evtype, author, evtime=otime, nordic_format=nordic_format)\n",
    "            sfile_path = os.path.join(ymdir, sfilename) \n",
    "            shutil.move(sfilename, sfile_path)\n",
    "\n",
    "catalog2rea(mergedcatalog2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the summit coordinates of Mount Pinatubo\n",
    "pinatubo_lon, pinatubo_lat = 120.35, 15.13\n",
    "print(pinatubo_lat, pinatubo_lon)\n",
    "\n",
    "# Try to read STATION0.HYP\n",
    "from importlib import reload\n",
    "import seisan_classes\n",
    "reload(seisan_classes)\n",
    "hypfile = os.path.join(SOURCE_DIR, 'station0.hyp')\n",
    "stationdf = seisan_classes.parse_STATION0HYP(hypfile)\n",
    "\n",
    "\n",
    "from obspy import Trace\n",
    "from libseisGT import fix_trace_id\n",
    "stationdf['seed_id'] = None\n",
    "netcode = 'XB'\n",
    "for i,row in stationdf.iterrows():\n",
    "    tr = Trace()\n",
    "    #tr.id = f\"{netcode}.{row['name'][0:3]}..EH{row['name'][3]}\"\n",
    "    tr.id = f\".{row['name']}..\"\n",
    "    tr.stats.sampling_rate=100.0\n",
    "    fix_trace_id(tr, legacy=True, netcode=netcode)\n",
    "    stationdf.at[i, 'seed_id'] = tr.id\n",
    "\n",
    "print(stationdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import pygmt\n",
    "\n",
    "# Extract unique stations (only first 3 characters of 'name')\n",
    "#df_unique_stations = stationdf.groupby(stationdf['name'].str[:3]).first().reset_index()\n",
    "df_unique_stations = stationdf.groupby(stationdf['name'].str[:3], as_index=False).first()\n",
    "\n",
    "# Define map region and projection\n",
    "region = [df_unique_stations['lon'].min() - 0.1, df_unique_stations['lon'].max() + 0.1,\n",
    "          df_unique_stations['lat'].min() - 0.1, df_unique_stations['lat'].max() + 0.1]\n",
    "resolution = '03s'\n",
    "projection = 'M4i'\n",
    "contour_interval=200\n",
    "ergrid = pygmt.datasets.load_earth_relief(resolution=resolution, region=region)\n",
    "fig = pygmt.Figure()\n",
    "\n",
    "pygmt.makecpt(\n",
    "    cmap='topo',\n",
    "    series='-1300/1800/%d' % contour_interval,\n",
    "    continuous=True\n",
    "    )\n",
    "\n",
    "fig.grdimage(\n",
    "    grid=ergrid,\n",
    "    region=region,\n",
    "    projection=projection,\n",
    "    shading=True,\n",
    "    frame=True\n",
    "    )\n",
    "\n",
    "fig.basemap(map_scale=\"g120.5/14.97+w10k+f\")\n",
    "\n",
    "# plot continents, shorelines, rivers, and borders\n",
    "fig.coast(\n",
    "    region=region,\n",
    "    projection=projection,\n",
    "    shorelines=True,\n",
    "    frame=True\n",
    "    )\n",
    "    \n",
    "# plot the topographic contour lines\n",
    "fig.grdcontour(\n",
    "    grid=ergrid,\n",
    "    interval=contour_interval,\n",
    "    annotation=\"%d+f6p\" % contour_interval,\n",
    "    limit=\"-1300/1800\", #to only display it below \n",
    "    pen=\"a0.15p\"\n",
    "    )\n",
    "    \n",
    "fig.colorbar(\n",
    "    frame='+lTopography (m)',\n",
    "    )\n",
    "\n",
    "# Add station names as labels\n",
    "fig.plot(x=df_unique_stations['lon'], y=df_unique_stations['lat'], style=\"s0.3c\", fill=\"black\", pen=\"black\")\n",
    "for _, row in df_unique_stations.iterrows():\n",
    "    fig.text(x=row['lon'], y=row['lat'] + 0.02, text=row['name'][:3], font=\"12p,Helvetica-Bold,black\", justify=\"CM\", fill=\"white@50\")\n",
    "\n",
    "fig.plot(x=pinatubo_lon, y=pinatubo_lat, style=\"t0.6c\", fill=\"red\", pen=\"red\")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import pygmt\n",
    "\n",
    "# Extract unique stations (only first 3 characters of 'name')\n",
    "#df_unique_stations = stationdf.groupby(stationdf['name'].str[:3]).first().reset_index()\n",
    "df_unique_stations = stationdf.groupby(stationdf['name'].str[:3], as_index=False).first()\n",
    "\n",
    "# Define map region and projection\n",
    "pad = 5.0\n",
    "region = [df_unique_stations['lon'].min() - pad, df_unique_stations['lon'].max() + pad,\n",
    "          df_unique_stations['lat'].min() - pad, df_unique_stations['lat'].max() + pad]\n",
    "resolution = '01m'\n",
    "projection = 'M4i'\n",
    "contour_interval=500\n",
    "ergrid = pygmt.datasets.load_earth_relief(resolution=resolution, region=region)\n",
    "fig = pygmt.Figure()\n",
    "pygmt.makecpt(\n",
    "    cmap='geo',\n",
    "    series='-1/1800/%d' % contour_interval,\n",
    "    continuous=True\n",
    "    )\n",
    "\n",
    "fig.grdimage(\n",
    "    grid=ergrid,\n",
    "    region=region,\n",
    "    projection=projection,\n",
    "    shading=True,\n",
    "    frame=True,\n",
    "    cmap=\"geo\",\n",
    "    )\n",
    "\n",
    "fig.basemap(map_scale=\"g116.4/11.5+w200k+f\")\n",
    "\n",
    "# plot continents, shorelines, rivers, and borders\n",
    "fig.coast(\n",
    "    #water='white',\n",
    "    region=region,\n",
    "    projection=projection,\n",
    "    shorelines=True,\n",
    "    frame=True,\n",
    "    )\n",
    "'''\n",
    "# plot the topographic contour lines\n",
    "fig.grdcontour(\n",
    "    grid=ergrid,\n",
    "    interval=contour_interval,\n",
    "    annotation=\"%d+f6p\" % contour_interval,\n",
    "    limit=\"-1300/1800\", #to only display it below \n",
    "    pen=\"a0.15p\"\n",
    "    )\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "# Add a red arrow pointing to the Pinatubo summit\n",
    "#fig.plot(x=[pinatubo_lon], y=[pinatubo_lat], style=\"v1c+b+h0.5\", direction=[[-180], [1.5]], fill=\"red\", pen=\"3p,red\")\n",
    "\n",
    "# Add a red label for \"Pinatubo\"\n",
    "#fig.text(x=pinatubo_lon - 3, y=pinatubo_lat, text=\"Pinatubo\", font=\"16p,Helvetica-Bold,red\", justify=\"CM\")    \n",
    "fig.text(x=pinatubo_lon - 2, y=pinatubo_lat, text=\"Pinatubo\", font=\"16p,Helvetica-Bold,red\", justify=\"CM\")  \n",
    "\n",
    "fig.colorbar(\n",
    "    frame='+lTopography (m)',\n",
    "    )\n",
    "\n",
    "fig.plot(x=pinatubo_lon, y=pinatubo_lat, style=\"t0.6c\", fill=\"red\", pen=\"red\")\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "passoft3_pygmt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
